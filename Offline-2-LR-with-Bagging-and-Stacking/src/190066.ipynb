{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "LABEL_COLUMN_NAME = 'label'\n",
    "SCALE_METHOD = 'standard'\n",
    "TOP_K = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility functions for data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values_and_duplicates(dataframe):\n",
    "    print(dataframe.isnull().sum())\n",
    "\n",
    "    # drop the rows where the target is missing\n",
    "    dataframe.dropna(subset=[LABEL_COLUMN_NAME], inplace=True)\n",
    "\n",
    "\n",
    "    # Fill missing values\n",
    "    for column in dataframe.columns:\n",
    "        if dataframe[column].dtype == 'object':  # Check if column is string\n",
    "            dataframe[column] = dataframe[column].fillna(dataframe[column].mode()[0])\n",
    "        else:  # Otherwise, it's a numeric column\n",
    "            dataframe[column] = dataframe[column].fillna(dataframe[column].mean())\n",
    "\n",
    "    #  Drop duplicates\n",
    "    dataframe.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Reset the index\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Show the number of attributes (columns) and number of records (rows)\n",
    "    print(\"Number of attributes: \", dataframe.shape[1])\n",
    "    print(\"Number of records: \", dataframe.shape[0])\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(features, scaler, columns_to_scale=None):\n",
    "    if scaler == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid scaler\")\n",
    "    \n",
    "    if columns_to_scale is None:\n",
    "        return scaler.fit_transform(features)\n",
    "    else:\n",
    "        # duplicate the features\n",
    "        features = features.copy()\n",
    "        features[columns_to_scale] = scaler.fit_transform(features[columns_to_scale])\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(dataframe, scaler=SCALE_METHOD):\n",
    "\n",
    "    features = dataframe.drop(LABEL_COLUMN_NAME, axis=1)\n",
    "\n",
    "    # get name of columns with datatype object\n",
    "    object_columns = features.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # convert the object columns to categorical\n",
    "    for col in object_columns:\n",
    "        features[col] = features[col].astype('category')\n",
    "\n",
    "\n",
    "\n",
    "    columns = features.columns\n",
    "\n",
    "    \n",
    "    # drop columns with only one value\n",
    "    columns_to_drop = [col for col in columns if features[col].nunique() == 1]\n",
    "    features = features.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "\n",
    "    # Encoding---------------------------------------------------------\n",
    "\n",
    "    # columns for label encoding - columns with 2 unique values\n",
    "    columns_to_label_encode = [col for col in columns if features[col].nunique() == 2]\n",
    "\n",
    "    # columns for one hot encoding - columns with more than 2 unique values from categorical columns\n",
    "    categorical_columns = features.select_dtypes(include=['category']).columns\n",
    "    columns_to_one_hot_encode = [col for col in categorical_columns if features[col].nunique() > 2]\n",
    "\n",
    "\n",
    "    # columns_to_one_hot_encode = columns_to_one_hot_encode + columns_to_label_encode\n",
    "    # columns_to_label_encode = []\n",
    "\n",
    "    # columns to scale - columns  - these three\n",
    "    columns_to_scale = list(set(columns) - set(columns_to_drop) - set(columns_to_label_encode) - set(columns_to_one_hot_encode))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # one hot encode columns\n",
    "    features = pd.get_dummies(features, columns=columns_to_one_hot_encode)\n",
    "\n",
    "    # label encode columns\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in columns_to_label_encode:\n",
    "        features[col] = label_encoder.fit_transform(features[col])\n",
    "\n",
    "    \n",
    "\n",
    "    # scaling -------------------------------------\n",
    "    features_scaled = scale_features(features, scaler, columns_to_scale)\n",
    "    features_df = pd.DataFrame(features_scaled, columns=features.columns)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Columns to drop: \", columns_to_drop)\n",
    "    print(\"Columns to label encode: \", columns_to_label_encode)\n",
    "    print(\"Columns to one hot encode: \", columns_to_one_hot_encode)\n",
    "    print(\"Columns to scale: \", columns_to_scale)\n",
    "\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labels(dataframe):\n",
    "    labels = dataframe[LABEL_COLUMN_NAME]\n",
    "    \n",
    "    # label encode the labels\n",
    "    encoder = LabelEncoder()\n",
    "    labels = encoder.fit_transform(labels)\n",
    "    \n",
    "\n",
    "    # scaled feature dataframe\n",
    "    labels_df = pd.DataFrame(labels, columns=[LABEL_COLUMN_NAME])\n",
    "\n",
    "    return labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_PATH = '../datasets/Telco-Customer-Churn/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "# LABEL_COLUMN_NAME = 'Churn'\n",
    "\n",
    "# dataframe = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "# # Show the number of attributes (columns) and number of records (rows)\n",
    "# print(\"Number of attributes: \", dataframe.shape[1])\n",
    "# print(\"Number of records: \", dataframe.shape[0])\n",
    "\n",
    "# dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop the customerID column\n",
    "# dataframe.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "# # Replace the cells with ' ' with np.nan\n",
    "# dataframe = dataframe.replace(' ', np.nan)\n",
    "\n",
    "# # Convert TotalCharges to numeric\n",
    "# dataframe['TotalCharges'] = pd.to_numeric(dataframe['TotalCharges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = handle_missing_values_and_duplicates(dataframe)\n",
    "# features_df = process_features(dataframe)\n",
    "# labels_df = process_labels(dataframe)\n",
    "# feature_names = features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = features_df.values\n",
    "# y = labels_df.values.flatten()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_PATH = '../datasets/adult/adult.data'\n",
    "# DATASET_PATH2 = '../datasets/adult/adult.test'\n",
    "# LABEL_COLUMN_NAME = 'class'\n",
    "\n",
    "# df_train = pd.read_csv(DATASET_PATH, header=None)\n",
    "# df_test = pd.read_csv(DATASET_PATH, header=None)\n",
    "\n",
    "\n",
    "# feature_name = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "#                 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "#                 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'class']\n",
    "\n",
    "# df_train.columns = feature_name\n",
    "# df_test.columns = feature_name\n",
    "\n",
    "\n",
    "# # Show the number of attributes (columns) and number of records (rows)\n",
    "# print(\"Number of attributes: \", df_train.shape[1])\n",
    "# print(\"Number of records: \", df_train.shape[0])\n",
    "\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = df_train.replace(' ?', np.nan)\n",
    "# df_test = df_test.replace(' ?', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = handle_missing_values_and_duplicates(df_train)\n",
    "# features_df = process_features(dataframe)\n",
    "# labels_df = process_labels(dataframe)\n",
    "# feature_names = features_df.columns\n",
    "\n",
    "# X = features_df.values\n",
    "# y = labels_df.values.flatten()\n",
    "\n",
    "# X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = handle_missing_values_and_duplicates(df_test)\n",
    "# features_df = process_features(dataframe)\n",
    "# labels_df = process_labels(dataframe)\n",
    "# feature_names = features_df.columns\n",
    "\n",
    "# X_test = features_df.values\n",
    "# y_test = labels_df.values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_PATH = '../datasets/Credit-Card-Fraud-Detection/creditcard.csv'\n",
    "# LABEL_COLUMN_NAME = 'Class'\n",
    "\n",
    "# dataframe = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "# # Show the number of attributes (columns) and number of records (rows)\n",
    "# print(\"Number of attributes: \", dataframe.shape[1])\n",
    "# print(\"Number of records: \", dataframe.shape[0])\n",
    "\n",
    "# dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of positive and negative samples\n",
    "# positive_samples = dataframe[dataframe[LABEL_COLUMN_NAME] == 1]\n",
    "# negative_samples = dataframe[dataframe[LABEL_COLUMN_NAME] == 0]\n",
    "\n",
    "# print(\"Number of positive samples: \", positive_samples.shape[0])\n",
    "# print(\"Number of negative samples: \", negative_samples.shape[0])\n",
    "\n",
    "\n",
    "# # Select a subset of the negative samples\n",
    "# negative_samples = negative_samples.sample(n=20000, random_state=42)\n",
    "\n",
    "\n",
    "# # Concatenate the positive and negative samples\n",
    "# dataframe = pd.concat([positive_samples, negative_samples], axis=0)\n",
    "\n",
    "# # Shuffle the dataset\n",
    "# dataframe = dataframe.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe = handle_missing_values_and_duplicates(dataframe)\n",
    "# features_df = process_features(dataframe)\n",
    "# labels_df = process_labels(dataframe)\n",
    "# feature_names = features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = features_df.values\n",
    "# y = labels_df.values.flatten()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point on, it is assumed that the data has been preprocessed and partitioned into train, validation and test sets in (X_train, y_train), (X_val, y_val) and (X_test, y_test) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For Dataset 3 - Oversampling the minority class\n",
    "\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# # Apply SMOTE to the training data\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Shuffle the resampled data\n",
    "# X_train_resampled, y_train_resampled = shuffle(X_train_resampled, y_train_resampled, random_state=42)\n",
    "\n",
    "# # Print the new class distribution\n",
    "# print(f\"Original training set shape: {X_train.shape}\")\n",
    "# print(f\"Resampled training set shape: {X_train_resampled.shape}\")\n",
    "# print(f\"Original training set positive class ratio: {np.sum(y_train) / len(y_train):.2%}\")\n",
    "# print(f\"Resampled training set positive class ratio: {np.sum(y_train_resampled) / len(y_train_resampled):.2%}\")\n",
    "\n",
    "\n",
    "# X_train = X_train_resampled\n",
    "# y_train = y_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top K features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_X_train , backup_y_train = X_train, y_train\n",
    "backup_X_validation, backup_y_validation = X_validation, y_validation\n",
    "backup_X_test, backup_y_test = X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 2000\n",
    "\n",
    "X_train = backup_X_train\n",
    "X_validation = backup_X_validation\n",
    "X_test = backup_X_test\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame(X_train)\n",
    "test_df = pd.DataFrame(X_test)\n",
    "validation_df = pd.DataFrame(X_validation)\n",
    "\n",
    "\n",
    "# add feature names\n",
    "train_df.columns = feature_names\n",
    "test_df.columns = feature_names\n",
    "validation_df.columns = feature_names\n",
    "\n",
    "\n",
    "df_labels = pd.DataFrame(y_train, columns=[LABEL_COLUMN_NAME])\n",
    "\n",
    "\n",
    "# correlation\n",
    "correlations = train_df.corrwith(df_labels[LABEL_COLUMN_NAME])\n",
    "\n",
    "top_k = correlations.abs().sort_values(ascending=False).head(min(TOP_K, len(correlations)))\n",
    "print(top_k)\n",
    "\n",
    "\n",
    "# take the top k features\n",
    "X_train = train_df[top_k.index].values\n",
    "X_test = test_df[top_k.index].values\n",
    "X_validation = validation_df[top_k.index].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Sensitivity Specificity Precision F1-score AUROC AUPR\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    # Print header\n",
    "    print(f\"{'Metric':<15} {'Value':>10}\")\n",
    "    print(\"-\" * 26)\n",
    "\n",
    "    # Print each metric\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{name:<15} {value:>10.4f}\")\n",
    "\n",
    "\n",
    "def print_mean_metrics(metrics):\n",
    "    # Print header\n",
    "    print(f\"{'Metric':<15} {'Mean':>10} {'std':>10}\")\n",
    "    print(\"-\" * 38)\n",
    "\n",
    "    # Print each metric\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{name:<15} {value['mean']:>10.4f} {value['std']:>10.4f}\")\n",
    "\n",
    "\n",
    "def print_report_format_metrics(metrics, row_name=\"\"):\n",
    "    print(row_name, end=\" | \")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{value:.4f}\", end=\" |\")\n",
    "    print()\n",
    "        \n",
    "\n",
    "def print_report_format_mean_metrics(metrics, row_name=\"\"):\n",
    "    # format mean +- std\n",
    "    print(row_name, end=\" | \")\n",
    "    for name, value in metrics.items():\n",
    "        print(f\"{value['mean']:.4f} Â± {value['std']:.4f}\", end=\" |\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def get_mean_metrics(metrics_list):\n",
    "    # calculate avarage and standard deviation of each metric\n",
    "    metrics = {}\n",
    "    for name in metrics_list[0].keys():\n",
    "        values = [m[name] for m in metrics_list]\n",
    "        metrics[name] = {\n",
    "            \"mean\": np.mean(values),\n",
    "            \"std\": np.std(values)\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    aupr = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Precision\": precision,\n",
    "        \"F1-score\": f1,\n",
    "        \"AUROC\": roc_auc,\n",
    "        \"AUPR\": aupr\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression of scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics = get_metrics(y_test, y_pred)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom implementation of Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "    def __init__(self, \n",
    "                 learning_rate=0.01, \n",
    "                 max_iter=1000,\n",
    "                 regularization=None,\n",
    "                 lambda_param=0.001,\n",
    "                 min_loss_diff=None, \n",
    "                 random_state=None, \n",
    "                 show_loss_curve=False):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = max_iter\n",
    "        self.min_loss_diff = min_loss_diff\n",
    "        self.weights = None\n",
    "        self.random_state = random_state\n",
    "        self.show_loss_curve = show_loss_curve\n",
    "        self.regularization = regularization\n",
    "        self.lambda_param = lambda_param\n",
    "\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        # x = np.array(x, dtype=np.float64)\n",
    "        return np.clip(1 / (1 + np.exp(-x)))\n",
    "    \n",
    "    # log likelihood\n",
    "    def _loss(self, y_pred, y_true):\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        log_loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        \n",
    "        if self.regularization == 'l1':\n",
    "            reg_term = self.lambda_param * np.sum(np.abs(self.weights))\n",
    "        elif self.regularization == 'l2':\n",
    "            reg_term = 0.5 * self.lambda_param * np.sum(self.weights**2)\n",
    "        else:\n",
    "            reg_term = 0\n",
    "        \n",
    "        return log_loss + reg_term\n",
    "\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # add 1 to the features\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "        y = np.asarray(y, dtype=np.float64)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        \n",
    "        # Initialize weights\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        self.weights = np.random.rand(n_features) * 0.01\n",
    "        \n",
    "\n",
    "        calculate_loss = self.show_loss_curve or self.min_loss_diff is not None\n",
    "        losses = np.zeros(self.n_iters)\n",
    "        max_iter_reached = 0\n",
    "\n",
    "        \n",
    "        for i in range(self.n_iters):\n",
    "            y_pred = self._sigmoid(X @ self.weights)\n",
    "            dw = (1 / len(X)) * (X.T @ (y_pred - y))\n",
    "\n",
    "            # add regularization\n",
    "            if self.regularization == 'l1':\n",
    "                dw += self.lambda_param * np.sign(self.weights)\n",
    "            elif self.regularization == 'l2':\n",
    "                dw += self.lambda_param * self.weights\n",
    "\n",
    "\n",
    "            self.weights = self.weights - self.learning_rate * dw\n",
    "\n",
    "            \n",
    "        \n",
    "            if calculate_loss:\n",
    "                loss = self._loss(y_pred, y)\n",
    "                losses[i] = loss\n",
    "\n",
    "                if self.min_loss_diff is not None:\n",
    "                    if i > 0 and abs(losses[i-1] - losses[i]) < self.min_loss_diff:\n",
    "                        max_iter_reached = i\n",
    "                        break\n",
    "        \n",
    "        # show loss curve\n",
    "        if self.show_loss_curve:\n",
    "            if max_iter_reached > 0:\n",
    "                plt.plot(range(max_iter_reached), losses[:max_iter_reached])\n",
    "            else:\n",
    "                plt.plot(range(self.n_iters), losses)\n",
    "            plt.xlabel('Iterations')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Loss Curve')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "\n",
    "        y_pred = self._sigmoid(X @ self.weights)\n",
    "        return y_pred\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) > threshold).astype(int)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BASE_LEARNER_REGULARIZATION = 'l1'\n",
    "BASE_LEARNER_LAMBDA_PARAM = 0.005\n",
    "BASE_LEARNER_MAX_ITER = 1000\n",
    "BASE_LEARNER_LEARNING_RATE = 0.1\n",
    "\n",
    "\n",
    "clf = CustomLogisticRegression(learning_rate=BASE_LEARNER_LEARNING_RATE,\n",
    "                                max_iter=BASE_LEARNER_MAX_ITER,\n",
    "                                regularization=BASE_LEARNER_REGULARIZATION,\n",
    "                                lambda_param=BASE_LEARNER_LAMBDA_PARAM,\n",
    "                                show_loss_curve=True,\n",
    "                                random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# test on validation and tune hyperparameters\n",
    "y_pred = clf.predict(X_validation)\n",
    "metrics = get_metrics(y_validation, y_pred)\n",
    "print_metrics(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on validation and tune hyperparameters\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics = get_metrics(y_test, y_pred)\n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def generate_bagging_sets(X, y, n_bags, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "    bags = []\n",
    "    for _ in range(n_bags):\n",
    "        X_resampled, y_resampled = resample(X, y, n_samples=len(X), replace=True)\n",
    "        bags.append((X_resampled, y_resampled))\n",
    "    return bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags = generate_bagging_sets(X_train, y_train, n_bags=9, random_state=42)\n",
    "\n",
    "# Initialize an empty list to store individual classifiers\n",
    "bagging_classifiers = []\n",
    "\n",
    "# Train individual classifiers\n",
    "for x, y in bags:\n",
    "    # clf = LogisticRegression(random_state=42)\n",
    "    clf = CustomLogisticRegression(learning_rate=BASE_LEARNER_LEARNING_RATE,\n",
    "                                   max_iter=BASE_LEARNER_MAX_ITER,\n",
    "                                   regularization=BASE_LEARNER_REGULARIZATION,\n",
    "                                   lambda_param=BASE_LEARNER_LAMBDA_PARAM,\n",
    "                                   random_state=42)\n",
    "    clf.fit(x, y)\n",
    "    bagging_classifiers.append(clf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance of models using voting ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Make predictions using individual classifiers\n",
    "y_pred_individual = []\n",
    "indivisual_metrics = []\n",
    "for i, clf in enumerate(bagging_classifiers):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_individual.append(y_pred)\n",
    "    indivisual_metrics.append(get_metrics(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(\"Mean and standard deviation of metrics for individual classifiers\")\n",
    "mean_metrics = get_mean_metrics(indivisual_metrics)\n",
    "print_mean_metrics(mean_metrics)\n",
    "\n",
    "\n",
    "# Majority voting using mode\n",
    "y_pred = stats.mode(y_pred_individual).mode\n",
    "\n",
    "\n",
    "print(\"\\n\\nMetrics for majority voting\")\n",
    "metrics = get_metrics(y_test, y_pred)\n",
    "print_metrics(metrics)\n",
    "\n",
    "# for report copypasta\n",
    "print()\n",
    "print_report_format_mean_metrics(mean_metrics, \"LR\")\n",
    "print_report_format_metrics(metrics, \"Voting ensemble\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(indivisual_metrics)\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "df_melted = df.melt(var_name='Metric', value_name='Value')\n",
    "\n",
    "# Set a color palette for each metric\n",
    "palette = sns.color_palette(\"Set2\", len(df_melted['Metric'].unique()))\n",
    "\n",
    "# Create the violin plot with hue assigned and palette used\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.violinplot(x='Metric', y='Value', data=df_melted, hue='Metric', palette=palette, dodge=False, legend=False)\n",
    "plt.title('Performance Metrics for 9 Bagging LR Learners')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the meta model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  predict on validation set using the bagging classifiers\n",
    "y_pred_validation = np.zeros((X_validation.shape[0], len(bagging_classifiers)))\n",
    "for i, clf in enumerate(bagging_classifiers):\n",
    "    y_pred_validation[:,i] = clf.predict(X_validation)\n",
    "\n",
    "\n",
    "# add previous features with predictions of individual classifiers\n",
    "stacking_features = np.column_stack((X_validation, y_pred_validation))\n",
    "\n",
    "\n",
    "# Train a meta-model\n",
    "# meta_model = LogisticRegression(random_state=42)\n",
    "meta_model = CustomLogisticRegression(learning_rate=0.5, max_iter=1000, regularization=\"l2\", lambda_param=0.005, show_loss_curve=True, random_state=42)\n",
    "meta_model.fit(stacking_features, y_validation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance of models using stacking ensemble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction of individual classifiers on test set\n",
    "y_pred_test_bagging = np.zeros((X_test.shape[0], len(bagging_classifiers)))\n",
    "for i, clf in enumerate(bagging_classifiers):\n",
    "    y_pred_test_bagging[:,i] = clf.predict(X_test)\n",
    "\n",
    "# Prepare test data for stacking\n",
    "stacking_features_test = np.column_stack((X_test,y_pred_test_bagging))\n",
    "\n",
    "\n",
    "# Make predictions using the meta-model\n",
    "y_pred = meta_model.predict(stacking_features_test)\n",
    "metrics = get_metrics(y_test,y_pred)\n",
    "print_metrics(metrics)\n",
    "\n",
    "\n",
    "\n",
    "# For report copypasta\n",
    "print()\n",
    "print_report_format_metrics(metrics, \"Stacking ensemble\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
