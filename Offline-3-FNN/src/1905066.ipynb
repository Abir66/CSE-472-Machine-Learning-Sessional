{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n",
    "ReLU, Softmax, CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_logs(logs):\n",
    "    epochs = range(1, len(logs['train_loss']) + 1)\n",
    "\n",
    "    # Create subplots in a 2x2 grid\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Plot train loss\n",
    "    axs[0, 0].plot(epochs, logs['train_loss'], label='Train Loss', color='blue')\n",
    "    axs[0, 0].set_title('Training Loss')\n",
    "    axs[0, 0].set_xlabel('Epoch')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # Plot train accuracy\n",
    "    axs[0, 1].plot(epochs, logs['train_accuracy'], label='Train Accuracy', color='green')\n",
    "    axs[0, 1].set_title('Training Accuracy')\n",
    "    axs[0, 1].set_xlabel('Epoch')\n",
    "    axs[0, 1].set_ylabel('Accuracy')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # Plot validation loss\n",
    "    if logs['validation_f1']:\n",
    "        axs[1, 0].plot(epochs, logs['validation_f1'], label='Validation f1', color='orange')\n",
    "        axs[1, 0].set_title('Validation f1')\n",
    "        axs[1, 0].set_xlabel('Epoch')\n",
    "        axs[1, 0].set_ylabel('Loss')\n",
    "        axs[1, 0].legend()\n",
    "\n",
    "    # Plot validation accuracy\n",
    "    if logs['validation_accuracy']:\n",
    "        axs[1, 1].plot(epochs, logs['validation_accuracy'], label='Validation Accuracy', color='red')\n",
    "        axs[1, 1].set_title('Validation Accuracy')\n",
    "        axs[1, 1].set_xlabel('Epoch')\n",
    "        axs[1, 1].set_ylabel('Accuracy')\n",
    "        axs[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "class ReLU:\n",
    "    def forward(self, x, is_training=True):\n",
    "        self.X = x\n",
    "        return np.where(x > 0, x, 0)\n",
    "    \n",
    "    def backward(self, grad, learning_rate):\n",
    "        return grad * np.where(self.X > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x, is_training=True):\n",
    "        exp_x = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "        self.output = exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad, learning_rate):\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y, y_hat):\n",
    "    return -np.mean(y * np.log(y_hat + 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks of Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "\n",
    "    def __init__(self, prev_neurons=1, neurons=1):\n",
    "        self.W = np.random.randn(neurons, prev_neurons) *  np.sqrt(2 / prev_neurons)\n",
    "        self.b = np.zeros((neurons, 1))\n",
    "        self.neurons = neurons\n",
    "        self.prev_neurons = prev_neurons\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, X, is_training=True):\n",
    "        self.X = X\n",
    "        self.Z =  self.W @ X + self.b\n",
    "        return self.Z\n",
    "    \n",
    "    def add_optimizer(self):\n",
    "        self.optimizer = True\n",
    "        \n",
    "        \n",
    "    def backward(self, dZ, learning_rate, optimizer=None):\n",
    "        m = self.X.shape[1]\n",
    "        self.dW = (dZ @ self.X.T) / m\n",
    "        self.db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        \n",
    "        if not optimizer:\n",
    "            self.W -= learning_rate * self.dW\n",
    "            self.b -= learning_rate * self.db\n",
    "\n",
    "        return self.W.T @ dZ\n",
    "    \n",
    "    def save(self):\n",
    "        return self.W, self.b\n",
    "    \n",
    "    def load(self, params):\n",
    "        self.W = params[0]\n",
    "        self.b = params[1]\n",
    "        self.neurons = self.W.shape[1]\n",
    "        self.prev_neurons = self.W.shape[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, keep_prob=0.8):\n",
    "        self.keep_prob = keep_prob\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, X, is_training=True):\n",
    "        if is_training:\n",
    "            self.mask = np.random.rand(*X.shape) < self.keep_prob\n",
    "            self.mask = self.mask / self.keep_prob\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def backward(self, dA, learning_rate):\n",
    "        return dA * self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, neurons, momentum=0.9):\n",
    "        self.gamma = np.ones((neurons, 1))\n",
    "        self.beta = np.zeros((neurons, 1))\n",
    "        self.episilon = 1e-15\n",
    "        self.momentum = momentum\n",
    "        self.running_mean = np.zeros((neurons, 1))\n",
    "        self.running_var = np.zeros((neurons, 1))\n",
    "        \n",
    "    def forward(self, X, is_training=True):\n",
    "        if is_training:\n",
    "            self.x = X\n",
    "            self.mean = np.mean(X, axis=1, keepdims=True)\n",
    "            self.var = np.var(X, axis=1, keepdims=True)\n",
    "            self.X_norm = (X - self.mean) / np.sqrt(self.var + self.episilon)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.var\n",
    "        else:\n",
    "            self.X_norm = (X - self.running_mean) / np.sqrt(self.running_var + self.episilon)\n",
    "        return self.gamma * self.X_norm + self.beta\n",
    "    \n",
    "    def backward(self, dA, learning_rate):\n",
    "        m = self.x.shape[1]\n",
    "        dgamma = np.sum(dA * self.X_norm, axis=1, keepdims=True) / m\n",
    "        dbeta = np.sum(dA, axis=1, keepdims=True) / m\n",
    "\n",
    "        dX_norm = dA * self.gamma\n",
    "        dvar = np.sum(dX_norm * (self.x - self.mean) * -0.5 * np.power(self.var + self.episilon, -1.5), axis=1, keepdims=True)\n",
    "        dmean = np.sum(dX_norm * -1 / np.sqrt(self.var + self.episilon), axis=1, keepdims=True) + dvar * np.sum(-2 * (self.x - self.mean), axis=1, keepdims=True) / m\n",
    "        \n",
    "        dX = dX_norm / np.sqrt(self.var + self.episilon) + dvar * 2 * (self.x - self.mean) / m + dmean / m\n",
    "        \n",
    "        self.gamma -= learning_rate * dgamma\n",
    "        self.beta -= learning_rate * dbeta\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "    def save(self):\n",
    "        return self.gamma, self.beta, self.running_mean, self.running_var\n",
    "    \n",
    "    def load(self, params):\n",
    "        self.gamma, self.beta, self.running_mean, self.running_var = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, layers, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers = layers\n",
    "        self.m_w = [np.zeros_like(layer.W) for layer in layers if isinstance(layer, DenseLayer)]\n",
    "        self.v_w = [np.zeros_like(layer.W) for layer in layers if isinstance(layer, DenseLayer)]\n",
    "        self.m_b = [np.zeros_like(layer.b) for layer in layers if isinstance(layer, DenseLayer)]\n",
    "        self.v_b = [np.zeros_like(layer.b) for layer in layers if isinstance(layer, DenseLayer)]\n",
    "        self.t = 0\n",
    "\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, DenseLayer):\n",
    "                layer.add_optimizer()\n",
    "\n",
    "    def update(self):\n",
    "        self.t += 1\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, DenseLayer):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * layer.dW\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * layer.dW**2\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1**self.t)\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2**self.t)\n",
    "                layer.W -= self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * layer.db\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * layer.db**2\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1**self.t)\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2**self.t)\n",
    "                layer.b -= self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "                i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.last_layer_neurons = None\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        if isinstance(layer, DenseLayer):\n",
    "            assert self.last_layer_neurons is None or self.last_layer_neurons == layer.prev_neurons\n",
    "            self.last_layer_neurons = layer.neurons\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def add_dense_layer(self, neurons):\n",
    "        assert self.last_layer_neurons is not None\n",
    "        self.add_layer(DenseLayer(self.last_layer_neurons, neurons))\n",
    "\n",
    "    def add_dropout(self, keep_prob):\n",
    "        assert self.last_layer_neurons is not None\n",
    "        self.add_layer(Dropout(keep_prob))\n",
    "        \n",
    "    def add_batch_normalization(self):\n",
    "        assert self.last_layer_neurons is not None\n",
    "        self.add_layer(BatchNormalization(self.last_layer_neurons))\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self, X, is_training=True):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X, is_training)\n",
    "        return X\n",
    "    \n",
    "    def backward(self, dA, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA, learning_rate)\n",
    "        return dA\n",
    "        \n",
    "    def predict(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X, is_training=False)\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, path):\n",
    "        # don't just save the whole model. only save the weights and biases\n",
    "        layers = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, DenseLayer) or isinstance(layer, BatchNormalization):\n",
    "                layers.append(\n",
    "                    {\n",
    "                        'arch' : layer.__class__.__name__,\n",
    "                        'params' : layer.save()\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "                \n",
    "            else:\n",
    "                layers.append({'arch' : layer.__class__.__name__})\n",
    "\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(layers, f)\n",
    "\n",
    "\n",
    "    def load_model(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            layers = pickle.load(f)\n",
    "            for layer in layers:\n",
    "                if layer['arch'] == 'DenseLayer':\n",
    "                    self.add_layer(DenseLayer())\n",
    "                    self.layers[-1].load(layer['params'])\n",
    "                \n",
    "                elif layer['arch'] == 'BatchNormalization':\n",
    "                    self.add_batch_normalization()\n",
    "                    self.layers[-1].load(layer['params'])\n",
    "\n",
    "                else:\n",
    "                    self.add_layer(globals()[layer['arch']]())\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, cm=False):\n",
    "    \n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    for X, y in dataloader:  \n",
    "        # Prepare batch\n",
    "        X = X.view(X.shape[0], -1).numpy().T\n",
    "        y = y.numpy()\n",
    "        \n",
    "        # Forward pass\n",
    "        A = model.predict(X)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        y_pred = np.argmax(A, axis=0)\n",
    "        \n",
    "        preds.append(y_pred)\n",
    "        targets.append(y)\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    targets = np.concatenate(targets)\n",
    "    \n",
    "    accuracy = accuracy_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds, average='macro')\n",
    "    \n",
    "\n",
    "    if cm:\n",
    "        cm = confusion_matrix(targets, preds)\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.show()\n",
    "    \n",
    "    return accuracy, f1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "   \n",
    "    def train(self,model, train_dataloader, validation_dataloader=None,  epochs=10, batch_size=32, learning_rate=0.01, loss_fn=cross_entropy_loss):\n",
    "        \n",
    "        logs = {\n",
    "            'train_loss': [],\n",
    "            'train_accuracy': [],\n",
    "            'validation_accuracy': [],\n",
    "            'validation_f1': []\n",
    "        }\n",
    "\n",
    "\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.validation_dataloader = validation_dataloader\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = None\n",
    "        self.optimizer = AdamOptimizer(model.layers, learning_rate)\n",
    "      \n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            total_accuracy = 0\n",
    "            n_batches = 0\n",
    "\n",
    "            with tqdm(total=len(self.train_dataloader), desc=f\"Epoch {epoch+1}/{self.epochs}\") as pbar:\n",
    "                for X, y in train_dataloader:\n",
    "                    \n",
    "                    # Prepare batch\n",
    "                    X = X.view(X.shape[0], -1).numpy().T\n",
    "                    y = y.numpy()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    A = self.model.forward(X)\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    y_onehot = np.eye(10)[y].T\n",
    "                    loss = self.loss_fn(y_onehot, A)\n",
    "                    total_loss += loss\n",
    "\n",
    "                    # Backward pass\n",
    "                    dZ = A - y_onehot\n",
    "                    self.model.backward(dZ, self.learning_rate)\n",
    "                    \n",
    "                    if self.optimizer:\n",
    "                        self.optimizer.update()\n",
    "\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    y_pred = np.argmax(A, axis=0)\n",
    "                    accuracy = np.mean(y_pred == y)\n",
    "\n",
    "                    total_accuracy += accuracy\n",
    "                    n_batches += 1\n",
    "                    \n",
    "                    # tqdm show the loss and accuracy\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix({'Loss': total_loss/n_batches, 'Accuracy': total_accuracy/n_batches})\n",
    "                \n",
    "\n",
    "                # Compute validation loss and accuracy\n",
    "                if validation_dataloader is not None:\n",
    "                    validation_accuracy, validation_f1 = evaluate(self.model, validation_dataloader)\n",
    "                    print(f\"Epoch {epoch+1}/{self.epochs} - Loss: {total_loss/n_batches:.4f} - Accuracy: {total_accuracy/n_batches:.4f} - Validation accuracy: {validation_accuracy:.4f} - Validation F1: {validation_f1:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch+1}/{self.epochs} - Loss: {total_loss/n_batches:.4f} - Accuracy: {total_accuracy/n_batches:.4f}\")\n",
    "                    \n",
    "\n",
    "                # Save logs\n",
    "                logs['train_loss'].append(total_loss/n_batches)\n",
    "                logs['train_accuracy'].append(total_accuracy/n_batches)\n",
    "                if validation_dataloader is not None:\n",
    "                    logs['validation_accuracy'].append(validation_accuracy)\n",
    "                    logs['validation_f1'].append(validation_f1)\n",
    "                    \n",
    "        plot_training_logs(logs)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "    \n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# take 20% of the training data as validation data\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# data loaders\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "model.add_layer(DenseLayer(28*28, 512))\n",
    "model.add_batch_normalization()\n",
    "model.add_layer(ReLU())\n",
    "model.add_dropout(0.5)\n",
    "\n",
    "model.add_dense_layer(128)\n",
    "model.add_batch_normalization()\n",
    "model.add_layer(ReLU())\n",
    "model.add_dropout(0.8)\n",
    "\n",
    "model.add_dense_layer(10)\n",
    "model.add_batch_normalization()\n",
    "model.add_layer(Softmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()\n",
    "trainer.train(model, train_dataloader, val_dataloader,\n",
    "               epochs=1, batch_size=32, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1 = evaluate(model, test_dataloader)\n",
    "print(f\"Test accuracy: {accuracy:.4f} - Test F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model and architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save_model('models/model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
